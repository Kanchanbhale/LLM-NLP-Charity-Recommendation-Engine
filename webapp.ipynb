{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHfvfssIl6Y2",
        "outputId": "cce6c52b-956a-476c-caa8-3815aed381bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.1 MB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 49.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 47.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 50.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 37.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 48.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 428 kB 49.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 130 kB 48.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 793 kB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 381 kB 50.6 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMFHV9foaMui",
        "outputId": "ba3d727b-47dd-4eea-f28c-abfa479e084a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 211 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3k --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtvY2bYYN9zS",
        "outputId": "de2ddfc4-3efe-46da-b9d9-94782cf63404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting myapp.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile myapp.py\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from newspaper import Article\n",
        "\n",
        "st.title(\"INSTAGIVE!\")\n",
        "st.header(\"A Charity Recommendation Engine\")\n",
        "\n",
        "\n",
        "import pandas as pd                     # for data manipulation and analysis\n",
        "\n",
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt        # object-oriented API for embedding plots into applications\n",
        "\n",
        "import matplotlib                       \n",
        "import numpy as np                     # used for working with arrays\n",
        "import missingno as msno               # provides a series of visualisations to understand the presence and distribution of missing data within a pandas dataframe\n",
        "import altair as alt                   # statistical visualization library\n",
        "from vega_datasets import data         \n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import re\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import nltk.stem\n",
        "\n",
        "\n",
        "def recommend_charities(metrics, cause, charity_data, n=3):\n",
        "\n",
        "    \"\"\"\n",
        "    The following code sorts a list of charities (of a given cause) on the basis of the above 3 metrics -- overall rating, financial rating and the rating foe accountability and transperancy.\n",
        "\n",
        "    Top 'n' (n is currently set to 3) charities in the 3 sorted lists are recommended.\n",
        "    \"\"\"\n",
        "\n",
        "    top_charities = {}\n",
        "\n",
        "    for metric in metrics:\n",
        "        top_charities[metric] = charity_data.groupby(['category']).get_group(cause).sort_values(by=[metric], ascending=False).head(n).copy()\n",
        "    \n",
        "\n",
        "    return top_charities\n",
        "\n",
        "\n",
        "charity_data = pd.read_csv('/content/complete_data.csv') # update path link here\n",
        "\n",
        "df = pd.read_csv('/content/charity_navigator.csv.txt')\n",
        "# charity_data =\n",
        "df.drop(['Unnamed: 0','charityid'], axis=1, inplace=True)     #no. ofcols available and which are dropped \n",
        "\n",
        "## Drop Empty Rows\n",
        "df.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "pic = pd.DataFrame(df['category'].value_counts())\n",
        "pic['name'] = pic.index.values.tolist()\n",
        "\n",
        "threshold = 671\n",
        "\n",
        "lst = []\n",
        "\n",
        "for class_index, group in df.groupby('category'):\n",
        "    if (threshold - len(group) > 0): # oversample\n",
        "        lst.append(group)\n",
        "        lst.append(group.sample(threshold - len(group), replace=True, random_state=1))\n",
        "\n",
        "    elif (threshold - len(group) < 0): # under-sample\n",
        "        lst.append(group.sample(threshold, replace=True, random_state=1))\n",
        "\n",
        "    else:\n",
        "        lst.append(group)\n",
        "\n",
        "df_balanced = pd.concat(lst)\n",
        "\n",
        "target_lst = ['animals', 'arts culture humanities' ,'community development',\n",
        "              'education','environment']\n",
        "\n",
        "df_balanced = df_balanced.loc[df_balanced['category'].isin(target_lst)]\n",
        "\n",
        "target = 'category'\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df[target])\n",
        "#print(le.classes_)\n",
        "df_balanced['target'] = le.transform(df_balanced[target]) \n",
        "\n",
        "# merge text to create a document\n",
        "df_balanced['corpus'] = df_balanced.mission + df_balanced.tagline + df_balanced.cause\n",
        "# drop other columns, convenience\n",
        "df_balanced.drop(['mission', 'tagline', 'cause'], axis=1, inplace=True)\n",
        "\n",
        "target_to_category = {\n",
        "    0: \"animals\",\n",
        "    1: \"arts culture humanities\",\n",
        "    2: \"community development\",\n",
        "    3: \"education\",\n",
        "    4: \"environment\"\n",
        "}\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_balanced.corpus)\n",
        "\n",
        "# preprocessing step \n",
        "def drop_integers(s):\n",
        "    return re.sub(r'\\d+', '', s)\n",
        "\n",
        "# stemmer\n",
        "english_stemmer = nltk.stem.SnowballStemmer('english')       #visualization\n",
        "\n",
        "def stemmer(doc):\n",
        "    return [porter_stemmer.stem(w) for w in analyzer(doc)]\n",
        "\n",
        "class StemmedCountVectorizer(CountVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
        "        # will need to rewrite if pickled - due to lambda\n",
        "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
        "\n",
        "vectorizer_s = StemmedCountVectorizer(min_df=5,  preprocessor=drop_integers,\n",
        "                                      analyzer='word', stop_words='english') \n",
        "\n",
        "X = vectorizer_s.fit_transform(df_balanced.corpus)\n",
        "\n",
        "vectorizer = TfidfTransformer()\n",
        "X_tfidf = vectorizer.fit_transform(X)    #add vis. add to results, print words\n",
        "\n",
        "seed = 2\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_balanced.corpus, df_balanced.target,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=seed, \n",
        "                                                    shuffle=True)\n",
        "\n",
        "NB_pipeline = Pipeline([\n",
        "    ('vect', StemmedCountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB(alpha=0.01)),\n",
        "])\n",
        "\n",
        "LR_pipeline = Pipeline([\n",
        "    ('vect', StemmedCountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LogisticRegression(random_state=0, solver='lbfgs',\n",
        "                               multi_class='multinomial')),\n",
        "])\n",
        "\n",
        "SVC_pipeline = Pipeline([\n",
        "    ('vect', StemmedCountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "RF_pipeline = Pipeline([\n",
        "    ('vect', StemmedCountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', RandomForestClassifier(n_estimators=100, \n",
        "                                   max_depth=2)),\n",
        "])\n",
        "\n",
        "SGD_pipeline = Pipeline([\n",
        "    ('vect', StemmedCountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(max_iter=1000, tol=1e-3)),        \n",
        "])\n",
        "\n",
        "## naive bayes\n",
        "NB_pipeline.fit(X_train, y_train)\n",
        "predictionNB = NB_pipeline.predict(X_test)\n",
        "\n",
        "## Logistic Regression\n",
        "LR_pipeline.fit(X_train, y_train)\n",
        "predictionLR = LR_pipeline.predict(X_test)\n",
        "\n",
        "## Suport Vector\n",
        "SVC_pipeline.fit(X_train, y_train)\n",
        "predictionSVC = SVC_pipeline.predict(X_test)\n",
        "\n",
        "##Stochastic Gradient Descent\n",
        "SGD_pipeline.fit(X_train,y_train)\n",
        "predictionSGD = SGD_pipeline.predict(X_test)\n",
        "\n",
        "# random forest\n",
        "RF_pipeline.fit(X_train,y_train)\n",
        "predictionRF = RF_pipeline.predict(X_test)\n",
        "\n",
        "results = {'Algorithm': ['naive_bayes', 'logistic_regression','support_vector','gradient_descent','random_forest' ],\n",
        "           'Accuracy': [accuracy_score(y_test, predictionNB),accuracy_score(y_test, predictionLR),accuracy_score(y_test, predictionSVC),accuracy_score(y_test, predictionSGD),accuracy_score(y_test, predictionRF)] }\n",
        "res_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "link=st.text_input(\"Enter the Web-Article link:\",\"\")\n",
        "st.markdown(f\"Article link is: {link}\")\n",
        "st.markdown(f\"Type of input link {type(link)}\")\n",
        "\n",
        "if (link == ''): st.markdown(\"Empty link.\")\n",
        "\n",
        "article = Article(link)\n",
        "article.download()\n",
        "article.parse()\n",
        "article.nlp()\n",
        "\n",
        "article_text=article.text\n",
        "\n",
        "\n",
        "if (article_text == ''): st.markdown(\"Empty text.\")\n",
        "results=[]\n",
        "results = RF_pipeline.predict([article_text])\n",
        "#cause= target_to_category[results]\n",
        "#print(results)\n",
        "ans=\"\"\n",
        "\n",
        "for i in results:\n",
        "       ans=target_to_category[i]\n",
        "st.markdown(f\"\"\" ### The predicted cause is: {ans}\"\"\")\n",
        "\n",
        "metrics = ['overall_rating', 'financial_rating', 'accountability_and_transperancy_rating']\n",
        "\n",
        "recommended_charities = recommend_charities(metrics, ans, charity_data)\n",
        "\n",
        "for metric in metrics:\n",
        "    donation_link_list = recommended_charities[metric]['donation_link'].tolist()\n",
        "    webadress_list = recommended_charities[metric]['web_address'].tolist()\n",
        "    name_list = recommended_charities[metric]['charity_name'].tolist()\n",
        "    # output = \"Depending on \"+str(metric)+\" : \" + \" \".join(charity_list)\n",
        "    output = \"Depending on \"+str(metric)+\" :\"\n",
        "    st.markdown(output)\n",
        "\n",
        "    for idx in range(3):\n",
        "        st.write(\"[{name}]({web_address}). Click [here]({donation_link}) to donate.\".format(name=name_list[idx], web_address=webadress_list[idx], donation_link=donation_link_list[idx]))\n",
        "\n",
        "st.markdown(f\"Article text: {article_text}\")\n",
        "\n",
        "#print(\"The predicted cause is: {cause}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU0XhVBHz5-j",
        "outputId": "c1baec14-0eb2-4dd7-841b-ba24d403f436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-01 17:40:21.215 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.008s\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.229.150.75:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://all-emus-travel-35-229-150-75.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        " !streamlit run myapp.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVQbxzaZHotR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "webapp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}